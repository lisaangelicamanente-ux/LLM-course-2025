{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaangelicamanente-ux/LLM-course-2025/blob/main/week-2/in-context-learning/in_context_learning_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwK6PWBl7uQ5"
      },
      "source": [
        "# In-Context Learning\n",
        "\n",
        "\n",
        "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
        "\n",
        "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
        "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
        "\n",
        "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
        "* a search engine is used to retrieve some relevant information\n",
        "* that information is then provided to the LLM as context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtmjU9fG7uRA"
      },
      "source": [
        "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTuHggJ07uRB",
        "outputId": "1dea1f9b-1f85-4a23-eb77-c4f71db95ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.12/dist-packages (0.0.2)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests bs4 google-generativeai pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "j5VpSpBn7uRC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#import google.generativeai as genai\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from IPython.display import Markdown, display\n",
        "from pypdf import PdfReader\n",
        "from datetime import date\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iGGI6P_S7uRD"
      },
      "outputs": [],
      "source": [
        "#API_KEY = os.environ.get(\"nuova\")\n",
        "#API_KEY = userdata.get('nuova')\n",
        "#genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFkCDWtv7uRD"
      },
      "source": [
        "We select those papers that have been featured in Hugging Face papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "H65bZA_z7uRE"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://huggingface.co/papers\"\n",
        "page = requests.get(BASE_URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "h3s = soup.find_all(\"h3\")\n",
        "\n",
        "papers = []\n",
        "\n",
        "for h3 in h3s:\n",
        "    a = h3.find(\"a\")\n",
        "    title = a.text\n",
        "    link = a[\"href\"].replace('/papers', '')\n",
        "\n",
        "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})\n",
        "    papers = papers[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcNHID817uRE"
      },
      "source": [
        "Code to extract text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "M2bZMQY-7uRF"
      },
      "outputs": [],
      "source": [
        "def extract_paper(url):\n",
        "    html = urlopen(url).read()\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_pdf(url):\n",
        "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
        "    reader = PdfReader(\"pdf_file.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def extract_pdf_limited(url, max_chars=12000):\n",
        "    text = extract_pdf(url)\n",
        "    return text[:max_chars]\n",
        "\n",
        "text = extract_pdf_limited(paper[\"url\"])\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwwg8rYw7uRF",
        "outputId": "3915905b-22c5-426c-d9f4-1cb3f797dcd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "#LLM = \"gemini-2.5-flash\"\n",
        "#model = genai.GenerativeModel(LLM)\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "LLM = \"google/flan-t5-base\"\n",
        "\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=LLM,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd_tHXma7uRG"
      },
      "source": [
        "We use Gemini to summarize the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU7aCdnc7uRG"
      },
      "outputs": [],
      "source": [
        "#for paper in tqdm(papers):\n",
        "    #try:\n",
        "     #   paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n",
        "    #except:\n",
        "     #   print(\"Generation failed\")\n",
        "      #  paper[\"summary\"] = \"Paper not available\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using an open-source Hugging Face model to summarize the papers."
      ],
      "metadata": {
        "id": "kqTRzMwzCc7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papers = papers[:3]\n",
        "\n",
        "for paper in tqdm(papers):\n",
        "    print(\"Starting generation...\")\n",
        "\n",
        "    prompt = (\n",
        "    \"Task: Read the research paper below.\\n\\n\"\n",
        "    \"List exactly TWO strengths and TWO weaknesses.\\n\"\n",
        "    \"Use the following format exactly:\\n\\n\"\n",
        "    \"Strengths:\\n\"\n",
        "    \"- item 1\\n\"\n",
        "    \"- item 2\\n\\n\"\n",
        "    \"Weaknesses:\\n\"\n",
        "    \"- item 1\\n\"\n",
        "    \"- item 2\\n\\n\"\n",
        "    \"Paper:\\n\"\n",
        "    + extract_pdf_limited(paper[\"url\"], max_chars=2500)\n",
        ")\n",
        "\n",
        "\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=180,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    paper[\"summary\"] = result[0][\"generated_text\"]\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d15MlfMqF47v",
        "outputId": "a614aa1d-4284-4695-811e-dbce4efd626f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
            " 33%|███▎      | 1/3 [00:03<00:07,  3.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Starting generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [00:07<00:03,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Starting generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:13<00:00,  4.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "for paper in papers:\n",
        "    text = paper[\"summary\"]\n",
        "\n",
        "    strengths = \"\"\n",
        "    weaknesses = \"\"\n",
        "\n",
        "    if \"Strengths\" in text:\n",
        "        strengths = text.split(\"Strengths\", 1)[1]\n",
        "\n",
        "    if \"Weaknesses\" in strengths:\n",
        "        strengths, weaknesses = strengths.split(\"Weaknesses\", 1)\n",
        "\n",
        "    strengths = strengths.replace(\":\", \"\").strip()\n",
        "    weaknesses = weaknesses.replace(\":\", \"\").strip()\n",
        "\n",
        "    if strengths == \"\":\n",
        "        strengths = \"- Not clearly identified\"\n",
        "\n",
        "    if weaknesses == \"\":\n",
        "        weaknesses = \"- Not clearly identified\"\n",
        "\n",
        "    paper[\"table\"] = f\"\"\"\n",
        "| Strengths | Weaknesses |\n",
        "|----------|------------|\n",
        "| {strengths.replace('\\n', '<br>')} | {weaknesses.replace('\\n', '<br>')} |\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "euVlXa5vI5MD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for paper in papers:\n",
        "    printmd(f\"**[{paper['title']}]({paper['url']})**\\n\\n{paper['table']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "TF3zDF0II9Fg",
        "outputId": "20f71ae2-a79d-4901-93a6-b9e5dacef60d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/pdf/2512.10430)**\n\n\n| Strengths | Weaknesses |\n|----------|------------|\n| - item 1 - item 2 | - item 1 - item 2 |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/pdf/2512.10949)**\n\n\n| Strengths | Weaknesses |\n|----------|------------|\n| - item 1 - item 2 | - item 1 - item 2 |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/pdf/2512.10739)**\n\n\n| Strengths | Weaknesses |\n|----------|------------|\n| - item 1 - item 2 | - item 1 - item 2 Paper 2025-12-12 Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving Songyang Gao1* Yuzhe Gu1,2* Zijian Wu1,3* Lingkai Kong1,2* Wenwei Zhang1* Zhongrui Cai1 Fan Zheng4 Tianyou Ma1 Junhao Shen1,2 Haiteng Zhao1 Duanyang Zhang5 Huilun Zhang6 Kuikun Liu Chengqi Lyu Yanhui Duan1 Chiyu Chen1 Ningsheng Ma1 Jianfei Gao1 Han Lyu Dahua Lin1,3 Kai Chen1 |\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aNPcSTu7uRG"
      },
      "source": [
        "We print the results to a html file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SSMvy2dg7uRG"
      },
      "outputs": [],
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVTBGLg7uRH"
      },
      "source": [
        "We can also print the results to this notebook as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "rrvGD4Rj7uRH",
        "outputId": "d6ed0f9a-ba63-4ac4-d204-af4cb789f7bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/pdf/2512.10430)**<br>T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground Gen-T Team T-Tech, Moscow, Russia Correspondence: anatolii.s.potapov@gmail.com Abstract We introduceT-pro 2.0, an open-weight Rus- sian LLM for hybrid reasoning and efficient inference. The model supports direct answer- ing and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE-style speculative-decoding pipeline to reduce latency. To enable reproducible and extensi- ble research, we release the model weights, the T-Wix<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for paper in papers:\n",
        "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
        "                                                paper[\"url\"],\n",
        "                                                paper[\"summary\"]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}