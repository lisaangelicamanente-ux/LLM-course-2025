{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOirhl1phrk9ID+dBkPcZeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaangelicamanente-ux/LLM-course-2025/blob/main/week-4/supervised_finetuning_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0wBTw4gcCnr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqgO6UzIbPKU"
      },
      "source": [
        "# Fine-tuning a Large Language Model\n",
        "\n",
        "In this lecture we will be looking at how to fine-tune an existing pre-trained language model.\n",
        "\n",
        "## Learning outcomes\n",
        "* You will learn how to download a pre-trained model and a training dataset from Hugging Face.\n",
        "* You will learn how to fine-tune the downloaded model with the dataset using Hugging Face trl library and the supervised fine-tuning (SFT) method.\n",
        "* You will learn how to use the fine-tuned model to generate text based on user input / prompts.\n",
        "* You will learn how to upload the fine-tuned model to your own Hugging Face repository so that it can be used later or shared with other users.\n",
        "\n",
        "## Prerequistes\n",
        "* You will need the following free accounts: Google, Hugging Face and Weights & Biases. You may use your existing accounts or create new accounts for the purposes of this course.\n",
        "* We will use the [Hugging Face](https://huggingface.co/) libraries: transformers (for models), datasets (for datasets), trl (for training). We will also store the fine-tuned models in a Hugging Face repository.\n",
        "* Training is done using [Google Colab](https://colab.research.google.com/), which provides free access to Jupyter notebooks backed with a GPU compute required for fine-tuning.\n",
        "* For monitoring the training run we will use [Weights & Biases](https://wandb.ai/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0dlZMMibPKV"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4zYqX2mbPKV"
      },
      "source": [
        "Let's first install some pre-requisites using Python's package manager pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lioAFETZWlKi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft accelerate datasets trl wandb bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tnl2ieubPKW"
      },
      "source": [
        "Then we need to import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdjM7XroWqEx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer, TrainingArguments\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import notebook_login\n",
        "import torch\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGmF_ndJbPKW"
      },
      "source": [
        "We will download a pre-trained large language model from Hugging Face and a dataset to train the model with. Below we assign these to variables we will use later. We will also set the name of the repository and model for the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTq15hgUW8qB"
      },
      "outputs": [],
      "source": [
        "# Pre trained model (<7B)\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "\n",
        "HUGGING_FACE_USERNAME = \"lisaangelica\"  # <---- change to your hugging face username\n",
        "\n",
        "# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n",
        "new_model = f\"{HUGGING_FACE_USERNAME}/phi-2-sft-dolly\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztxz0D3sbPKW"
      },
      "source": [
        "To access your Hugging Face account, you need to log in. First go to your Hugging Face account, click *Settings* and select *Access Tokens*. Create a new token and copy the token. Then execute the below login command and when asked paste an access token.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnwXVf_dXeIp"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA_T4sazbPKX"
      },
      "source": [
        "Let's then download a subset of the dataset we want to use. Below we limit the dataset to the first 10,000 examples in order to save time. In real life you would probably use the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt_36Xa5Xuub"
      },
      "outputs": [],
      "source": [
        "# Load only a small subset of the dataset to fit within RAM constraints\n",
        "raw_dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Reformat each example into a single instruction-response text string\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
        "    }\n",
        "\n",
        "# Remove unused columns to reduce memory usage\n",
        "dataset = raw_dataset.map(format_example, remove_columns=raw_dataset.column_names)\n",
        "dataset = dataset.shuffle(seed=42).select(range(50)) # Further reduce dataset size for faster training and lower memory usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_10in4Y2bPKX"
      },
      "source": [
        "Let's then download the model. We first create a config object for quantization of the model using bitsandbytes. Bitsandbytes enables accessible large language models via k-bit quantization for PyTorch.\n",
        "\n",
        "We also need to download the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecbQfx6mYpSU"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.float16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"  # Load the base model with automatic device mapping to avoid GPU out-of-memory errors\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load the tokenizer and set EOS token as padding token for correct batching\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ7phW2nbPKX"
      },
      "source": [
        "Below we log in to Weights & Biases for experiment tracking.\n",
        "\n",
        "> * In Colab, store your key in the `WANDB_API_KEY` environment variable, or  \n",
        "> * Call `wandb.login()` and paste the key interactively when prompted.\n",
        ">\n",
        "> You can find your key in your [Weights & Biases account](https://wandb.ai/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow0n2oSOYz6V"
      },
      "outputs": [],
      "source": [
        "# Monitoring login (uses the WANDB_API_KEY environment variable if set)\n",
        "wandb.login()\n",
        "run = wandb.init(project=\"llm-finetuning-demo\", job_type=\"training\", anonymous=\"allow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lef15L4WbPKX"
      },
      "source": [
        "Then we'll create a configuration for the lo-rank adaptation method we will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D409OdMjcc-n"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.1,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFDA0ebUrmIA"
      },
      "source": [
        "#### LoRA Target Modules\n",
        "\n",
        "LoRA adds small trainable matrices into selected linear layers of a transformer.\n",
        "**Target modules** tell LoRA *which* layers to modify.\n",
        "\n",
        "**Common module names (LLaMA / Mistral / Qwen)**\n",
        "\n",
        "**Attention layers**\n",
        "\n",
        "* **q_proj**: creates attention *queries*\n",
        "* **k_proj**: creates attention *keys*\n",
        "* **v_proj**: creates attention *values*\n",
        "* **o_proj**: attention outputs\n",
        "\n",
        "**Feed-forward (MLP) layers**\n",
        "\n",
        "* **gate_proj**: gating in SwiGLU\n",
        "* **up_proj**: expands hidden size\n",
        "* **down_proj**: reduces back to model size\n",
        "\n",
        "**Recommended set for most models**\n",
        "\n",
        "```python\n",
        "[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "```\n",
        "\n",
        "**If VRAM is tight (e.g., T4)**\n",
        "\n",
        "```python\n",
        "[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "```\n",
        "\n",
        "These layers give the best trade-off between memory use and performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc6GMAJLbPKX"
      },
      "source": [
        "We need to set the training arguments for the training run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9sAp50mc4LR"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Where to save checkpoints & logs\n",
        "    num_train_epochs=1,              # Number of full passes through the dataset\n",
        "    per_device_train_batch_size=1,   # Batch size per GPU (before gradient accumulation) - Reduced to 1\n",
        "    gradient_accumulation_steps=8,   # Accumulate gradients to simulate a larger batch (1Ã—8 = 8)\n",
        "    optim=\"paged_adamw_8bit\",        # Memory-efficient optimizer from bitsandbytes (QLoRA-friendly)\n",
        "    save_steps=1000,                 # Save model every 1000 steps (set high to avoid slowing training)\n",
        "    logging_steps=10,                # Log metrics to W&B every 10 steps\n",
        "    learning_rate=2e-4,              # Base learning rate for training\n",
        "    weight_decay=0.001,              # Regularization to reduce overfitting\n",
        "    fp16=False,                      # Use float16 (disabled here)\n",
        "    bf16=False,                      # Use bfloat16 (disable on GPUs like T4 that don't support it)\n",
        "    max_grad_norm=0.3,               # Gradient clipping for training stability\n",
        "    max_steps=-1,                    # Train for full epochs (no manual step limit)\n",
        "    warmup_ratio=0.3,                # Fraction of steps for LR warmup (30%)\n",
        "    group_by_length=True,            # Buckets sequences by length for efficiency\n",
        "    lr_scheduler_type=\"linear\",      # Linear learning-rate schedule\n",
        "    report_to=\"wandb\",               # Send logs to Weights & Biases\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqGhuVdlbPKX"
      },
      "source": [
        "Finally we create the trainer object that uses supervised fine-tuning (SFT) as the training method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JdMo0sqc_SB"
      },
      "outputs": [],
      "source": [
        "# Setting SFT parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vNGH8Q5bPKX"
      },
      "source": [
        "Then, we can execute the training run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEiMWxk9dFLk"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZg7GbvHdOUc"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk81AuR4bPKY"
      },
      "outputs": [],
      "source": [
        "def stream(user_prompt: str):\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Works even with device_map=\"auto\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    )\n",
        "    B_INST, E_INST = \"### Instruction:\\n\", \"\\n\\n### Response:\\n\"\n",
        "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}{E_INST}\"\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Stream tokens directly to notebook output\n",
        "    streamer = TextStreamer(\n",
        "        tokenizer,\n",
        "        skip_prompt=True,          # don't print the full prompt\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        _ = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            streamer=streamer,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w3S8iH8bPKY"
      },
      "outputs": [],
      "source": [
        "stream(\"what is newtons 3rd law and its formula?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICmjOV1UbPKY"
      },
      "outputs": [],
      "source": [
        "# Same bnb_config as above, adjusted to match initial successful load\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.float16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0} # Changed from \"auto\" to match initial successful load\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "\n",
        "# Try merging LoRA into the base model\n",
        "model = model.merge_and_unload()  # may still be heavy on T4 depending on model size\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCWdMQurbPKY"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(new_model)\n",
        "tokenizer.push_to_hub(new_model)"
      ]
    }
  ]
}